<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>NICE-SLAM: Neural Implicit Scalable Encoding for SLAM</title>
    <meta name="description" content="Seongmin Jung's personal website" />
    <meta name="keywords" content="Seongmin Jung, Robotics" />
    <meta name="author" content="Seongmin Jung" />
    <meta name="language" content="English" />
    <link rel="shortcut icon" href="/favicon.png" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LL44K1WZ0G"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-LL44K1WZ0G");
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

    <!-- Load an icon library to show a hamburger menu (bars) on small screens -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <!-- import css -->
    <link rel="stylesheet" href="/css/index.css" />
    <!-- import js -->
    <script src="/js/script.js" defer></script>
  </head>

  <body>
    <header>
      <img class="cover" src="/asset/cover.jpg" alt="Seongmin Jung" />
      <h1 class="title"><a href="/index.html">Seongmin Jung</a></h1>
      <nav class="nav">
        <a href="/index.html">Home</a>
        <a href="/projects.html">Projects</a>
        <a href="/articles.html" class="bold">Articles</a>
        <a href="/blog.html">Blog</a>
        <i id="nav-toggle" class="nav-toggle fa fa-bars" onclick="toggleNav()"></i>
      </nav>
    </header>

    <div id="nav-modal-bg" onclick="toggleNav()"></div>
    <nav id="nav-modal">
      <a href="/index.html">Home</a>
      <a href="/projects.html">Projects</a>
      <a href="/articles.html" class="bold">Articles</a>
      <a href="/blog.html">Blog</a>
    </nav>

    <main>
      <div class="container">
        <section id="post">
          <a id="category-tag" href="/articles/paper_summaries.html"><i class="fa fa-book"></i> Paper Summaries</a>
          <h1>NICE-SLAM: Neural Implicit Scalable Encoding for SLAM</h1>
          <p class="date">Posted on <time datetime="2024-09-09">September 9, 2024</time></p>
          <div class="post-body">
            <h2>Introduction</h2>
            <p>
              In order to make a SLAM system useful for real-world applications, the following properties are essential:
            </p>
            <ul>
              <li>Real-time performance</li>
              <li>Ability to predict unseen areas</li>
              <li>Scalability</li>
              <li>Robustness</li>
            </ul>
            <p>
              However, traditional SLAM systems are unable to make plausible estimation for unobserved regions. On the
              other hand, learning-based SLAM systems lack scalability.
            </p>
            <p>
              This paper found multi-level grid-based features can help to address the scalability issue. Therefore,
              this paper combines hierarchical scene representation with neural implicit representations to achieve the
              desired properties above.
            </p>
            <p>
              The key idea is to represent the scene geometry and appearance with hierarchical feature grids and
              incorporate the inductive biases of neural implicit decoders pretrained at different spatial resolutions.
            </p>

            <h2>Related Works</h2>
            <h4>Dense Visual SLAM</h4>
            <p>
              Traditional SLAM systems store occupancy or TSDF values in voxel grids. However, these representations
              cannot predict unseen areas. NICE-SLAM also adopts the voxel-grid representation, however, NICE-SLAM
              stores implicit latent codes of the geometry and directly optimize them.
            </p>

            <h4>Neural Implicit Representations</h4>
            <p>
              Neural implicit representations are good at predicting and representing object geometry and appearance.
              However, some assume given camera poses and others are not suitable for real-time applications. iMAP
              shares the same goal as NICE-SLAM, but they use single MLP to represent the entire scene, which causes the
              forgetting issue. DI-Fusion also uses a feature grid, but it contains holes and tracking is not robust.
              NICE-SLAM can handle large-scale scenes and is robust to challenging scenarios.
            </p>

            <h2>Method</h2>
            <img src="/articles/paper_summaries/nice-slam/img1.png" alt="System Overview" />

            <h4>Hierarchical Scene Representation</h4>
            <p>
              The scene geometry is encoded into total four feature grids: 1) coarse-level, 2) mid-level, 3) fine-level,
              4) scene appearance. Four pre-trained neural implicit decoders are attached to each feature grid. These
              feature grids themselves act as the map.
            </p>
            <p>
              First, the system extracts the occupancy values by decoding the latent codes from the mid-level grid.
              Then, the system refines the geometry by decoding the latent codes from the fine-level grid.
            </p>
            <p>
              Second, the coarse-level grid is used to predict the appoximate occupancy values of the unseen areas. It
              aims to capture high-level geometry, such as walls and floors. The occupancy values are also decoded from
              the feature grid. The optimization of this coarse-level grid is independent from the other grids.
            </p>
            <p>
              Pre-training is done as a part of ConvONet. These pre-trained decoders are frozen during the training
              process to stabilize the optimization.
            </p>
            <p>
              Color information provides additional cues for tracking. Color is decoded by its own decoder from the
              color feature grid. This decoder is not frozen during the training, but is optimized during the mapping
              process. The color of the entire scene is represented by one decoder MLP, which may lead to the forgetting
              issue.
            </p>

            <h4>Depth and Color Rendering</h4>
            <p>
              The system uses a differentiable rendering process, similar to NeRF, to integrate the predicted occupancy
              and colors from the hierarchical feature grids. It first performs stratified sampling along the viewing
              ray, and also performs uniform sampling near the object surface.
            </p>
            <p>
              For every point, their coarse-level occupancy probability, fine-level occupancy probability, and color are
              computed using the pre-trained MLPs. Then, along the ray, the depth and color are integrated as the NeRF
              system does.
            </p>

            <h4>Mapping and Tracking</h4>
            <p>
              Depth loss and color loss are used to track the camera pose and optimize the feature grids. These losses
              are computed every frame. For each frame, the system samples total $M$ pixels from the current frame and
              the selected keyframes to calculate losses.
            </p>
            <p>
              The depth loss is an $L_1$ loss between the predicted depth of the current frame and the ground truth
              depth. The color loss is an $L_1$ loss between the predicted color of the current frame and the ground
              truth color.
            </p>
            <p>
              The camera tracking is formulated as the minimization problem that searches for the optimal camera pose
              $\{\mathbf{R},\mathbf{t}\}$ that minimizes the depth and color losses.
            </p>
            <p>
              For mapping, the system conducts a local bundle adjustment to jointly optimize feature grids, the color
              decoder, as well as the camera poses of all selected keyframes. This is also a minimization problem.
            </p>
            <p>Finally, the system filters out pixels with large depth/color losses to avoid dynamic objects.</p>

            <h4>Keyframe Selection</h4>
            <p>
              The system maintains a global keyframe list similar to iMAP. For mapping and tracking, however, only a
              subset of keyframes are selected for optimization. The keyframe selection strategy is as follows:
            </p>
            <ul>
              <li>The most recent keyframe is selected.</li>
              <li>
                $K-2$ Keyframes are randomly selected among those that have visual overlap with the current frame .
              </li>
            </ul>
            <p>
              Including the current frame, total $K$ frames are involved in the optimization process. Thanks to this
              keyframe selection strategy, the system is able to make local updates to the feature grids. That is, the
              geometry outside of the current view remains static. This results in a very efficient optmization process.
            </p>

            <h2>Experiments</h2>

            <h4>Experimental Setup</h4>
            <p>
              Datasets &ndash; Replica, ScanNet, TUM RGB-D, Co-Fusion, and a self-captured multi-room dataset are used
              for evaluation.
            </p>
            <p>
              Baselines &ndash; TSDF-Fusion, DI-Fusion, iMAP, and the re-implementation version of iMAP are used as
              baselines.
            </p>
            <p>
              Metrics &ndash; For 2D metric, the L1 loss is used on 1000 randomly-sampled points from both
              reconstruction and the ground truth. For 3D metrics, Accuracy [cm], Completion [cm], and Completion Ratio
              [< 5cm%] are used. For tracking metric, ATE RMSE is used.
            </p>

            <h4>Evaluation of Mapping and Tracking</h4>
            <img class="half" src="/articles/paper_summaries/nice-slam/img2.png" alt="Mapping Evaluation" />
            <p>
              In mapping, NICE-SLAM significantly outperforms baseline methods on almost all metrics, while keeping a
              reasonable memory consumption. Qualitatively, NICE-SLAM produces sharper geometry and less artifacts. The
              reconstruction results are shown in the figure above.
            </p>
            <img class="half" src="/articles/paper_summaries/nice-slam/img3.png" alt="Tracking Evaluation" />
            <p>
              The table above shows ATE RMSE of the tracking results on the TUM RGB-D dataset. NICE-SLAM outperforms all
              of the baselines in the neural implicit representation family. Although the traditional feature-based
              methods, such as ORB-SLAM2, outperform NICE-SLAM, NICE-SLAM reduces the gap between these two categories.
            </p>
            <img src="/articles/paper_summaries/nice-slam/img4.png" alt="Scalability Evaluation" />
            <img class="half" src="/articles/paper_summaries/nice-slam/img5.png" alt="Scalability Evaluation Table" />
            <p>
              The figure and table above shows the scalability evaluation results on ScanNet, which contains large-scale
              scenes. ATE RMSE is used for evaluation in the table.
            </p>
            <p>
              For the geometry shown in the figure above, NICE-SLAM produces sharper and more detailed geometry over the
              baseline methods.
            </p>
            <p>
              In terms of tracking, iMAP* and DI-Fusion either completely fails or introduces large drifting, while
              NICE-SLAM successfully reconstructs the entire scene. Quantitatively, as shown in the table above, the
              tracking results of NICE-SLAM are also significantly more accurate than the baseline methods .
            </p>
            <img src="/articles/paper_summaries/nice-slam/img6.png" alt="Larger Scenes" />
            <p>
              The figure above shows the reconstruction results of larger scenes from self-captured multi-room dataset.
              NICE-SLAM has comparable results with the offline method, while iMAP* and DI-Fusion fails to reconstruct
              the full sequence.
            </p>

            <h4>Performance Analysis</h4>
            <img class="half" src="/articles/paper_summaries/nice-slam/img7.png" alt="Performance Analysis" />
            <p>
              The table above compares the number of floating point operations (FLOPs) needed for one 3D point and the
              runtime for tracking and mapping using the same number of pixel samples.
            </p>
            <p>
              NICE-SLAM requires only 1/4 FLOPs of iMAP and is over 2x and 3x faster than iMAP in tracking and mapping.
              This indicates the advantage of using feature grids with shallow MLP decoders over a single heavy MLP.
            </p>
            <img
              class="half"
              src="/articles/paper_summaries/nice-slam/img8.png"
              alt="Geometry Forecast and Hole Filling"
            />
            <p>
              The white colored area is the region with observations, and cyan indicates the unobserved but predicted
              region. Thanks to the use of coarse-level scene prior, NICE-SLAM has better prediction capability compared
              to iMAP*. This in turn also improves the tracking performance.
            </p>

            <h4>Ablation Study</h4>
            <img
              class="half"
              src="/articles/paper_summaries/nice-slam/img9.png"
              alt="Hierarchical Architecture Ablation"
            />
            <p>
              The hierarchical architecture is first evaluated. The figure above shows the convergence trend of the
              hierarchical, high-only, and low-only architectures. The hierarchical architecture converges better.
            </p>
            <img class="half" src="/articles/paper_summaries/nice-slam/img10.png" alt="Ablation Study" />
            <p>
              The usefulness of local BA, color representation, and the keyframe selection strategy is also
              investigated. The table above shows the ATE RSME results of each ablation study. All three components play
              important roles in camera tracking.
            </p>

            <h2>Conclusion</h2>
            <p>
              NICE-SLAM is a dense visual SLAM system that combines hierarchical scene representation with neural
              implicit representations. The system is more scalable, fine-detailed, accurate, fast, and less
              computationally expensive thanks to local scene updates. Besides, NICE-SLAM is able to fill small holes
              and predict unobserved regions which stabilizes the camera tracking.
            </p>
            <p>
              There are also some limitations. The predictive ability of NICE-SLAM is restricted to the scale of the
              coarse representation. In addition, NICE-SLAM does not perform loop closures.
            </p>
          </div>
          <div class="post-footer">
            <div class="post-footer-profile">
              <img src="/asset/cover.jpg" alt="Seongmin Jung" />
              <h1>Seongmin Jung</h1>
            </div>
            <h2>Other posts in <a href="/articles/paper_summaries.html">Paper Summaries</a> category</h2>
            <ul>
              <li>
                <a href="/articles/paper_summaries/nice-slam.html">
                  <span><b>NICE-SLAM: Neural Implicit Scalable Encoding for SLAM</b></span>
                  <time datetime="2024-09-09">September 9, 2024</time>
                </a>
              </li>
              <li>
                <a href="/articles/paper_summaries/orb-slam.html">
                  <span>ORB-SLAM: a Versatile and Accurate Monocular SLAM System</span>
                  <time datetime="2024-08-28">August 28, 2024</time>
                </a>
              </li>
              <li>
                <a href="/articles/paper_summaries/kiss-icp.html">
                  <span
                    >KISS-ICP: In Defense of Point-to-Point ICP -- Simple, Accurate, and Robust Registration If Done the
                    Right Way</span
                  >
                  <time datetime="2023-12-18">December 18, 2023</time>
                </a>
              </li>
              <li>
                <a href="/articles/paper_summaries/cnn-slam.html">
                  <span>CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction</span>
                  <time datetime="2023-12-12">December 12, 2023</time>
                </a>
              </li>
              <li>
                <a href="/articles/paper_summaries/vmap.html">
                  <span>vMAP: Vectorised Object Mapping for Neural Field SLAM</span>
                  <time datetime="2023-12-05">December 5, 2023</time>
                </a>
              </li>
            </ul>
          </div>
        </section>
      </div>
    </main>

    <footer>
      <p>&copy; 2024 Seongmin Jung<br />Designed and developed by Seongmin Jung</p>
    </footer>
  </body>
</html>
