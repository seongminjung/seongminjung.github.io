<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Gaussian Grouping: Segment and Edit Anything in 3D Scenes</title>
    <meta name="description" content="Seongmin Jung's personal website" />
    <meta name="keywords" content="Seongmin Jung, Robotics" />
    <meta name="author" content="Seongmin Jung" />
    <meta name="language" content="English" />
    <link rel="shortcut icon" href="/favicon.png" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LL44K1WZ0G"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-LL44K1WZ0G");
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

    <!-- Load an icon library to show a hamburger menu (bars) on small screens -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <!-- import css -->
    <link rel="stylesheet" href="/css/index.css" />
    <!-- import js -->
    <script type="module" src="/js/main.js" defer></script>
  </head>

  <body>
    <header>
      <img class="cover" src="/asset/cover.jpg" alt="Seongmin Jung" />
      <h1 class="title"><a href="/index.html">Seongmin Jung</a></h1>
      <nav class="nav">
        <a href="/index.html">Home</a>
        <a href="/projects.html">Projects</a>
        <a href="/articles.html" class="bold">Articles</a>
        <a href="/blog.html">Blog</a>
        <i id="nav-toggle" class="nav-toggle fa fa-bars" onclick="toggleNav()"></i>
      </nav>
    </header>

    <div id="nav-modal-bg" onclick="toggleNav()"></div>
    <nav id="nav-modal">
      <a href="/index.html">Home</a>
      <a href="/projects.html">Projects</a>
      <a href="/articles.html" class="bold">Articles</a>
      <a href="/blog.html">Blog</a>
    </nav>

    <main>
      <div class="container">
        <section id="post">
          <post-header></post-header>

          <div class="post-body">
            <arxiv-card
              title="Gaussian Grouping: Segment and Edit Anything in 3D Scenes"
              venue="ECCV 2024"
              authors="Mingqiao Ye, Martin Danelljan, Fisher Yu, Lei Ke"
              link="https://arxiv.org/abs/2312.00732"
            >
            </arxiv-card>
            <h2>Introduction</h2>
            <img src="/articles/paper_summaries/gaussian_grouping/img1.png" alt="Introduction" />
            <p>
              본 논문의 목적은 3D reconstruction과 segmentation을 동시에 할 수 있는 3D representation을 학습하는 것이다.
              하지만 3D에서 바로 작동하는 모델을 만들기에는 대규모 3D annotation dataset을 구축하는 것에 비용적 한계가
              있다.
            </p>
            <p>
              기존에 NeRF rendering을 통해 CLIP, DINO 등 2D VLM feature를 distillation하여 위에 언급한 목적을 달성하려는
              모델도 있었지만, NeRF 모델은 렌더링 속도가 느리고 수정이 어렵다는 한계가 있다.
            </p>
            <p>
              이에 따라 본 논문에서는 기존의 3D Gaussian Splatting을 확장해서 object segmentation까지 수행할 수 있는
              모델을 구축하였다. 2D VLM 모델로부터 distillation하기 위해 각 가우시안에 Identity Encoding이라는
              low-dimensional embedding을 더해 각 가우시안이 속한 물체에 대한 정보를 학습하도록 했고, un-supervised 3D
              Regularization Loss를 새롭게 도입하여 공간상 가까이 위치하는 가우시안들이 유사한 identity encoding 값을
              학습할 수 있도록 했다.
            </p>
            <p>
              3D gaussian splatting의 explicit한 특성과 본 논문의 contribution 덕분에 Gaussian Grouping은 위 사진과 같이
              object removal, inpainting 등 3D scene editing을 지원한다.
            </p>

            <h2>Method</h2>
            <p>
              본 논문의 목적은 scene reconstruction뿐만 아니라 panoptic segmentation까지 가능한 모델을 만드는 것이다.
              Gaussian Grouping 모델은 이 목적을 달성하면서 각 물체를 group으로 구분지어 scene editing을 가능하게 하며,
              기존 3DGS의 품질을 유지하면서도 빠른 training과 rendering이 가능하다.
            </p>

            <h4>Preliminaries: 3D Gaussian Splatting</h4>
            <p>각 가우시안은 아래의 파라미터로 구성된다.</p>
            <ul>
              <li>중심점: $\mathbf{p} = \{x, y, z\} \in \mathbb{R}^3$</li>
              <li>표준편차 (크기): $\mathbf{s} \in \mathbb{R}^3$</li>
              <li>회전: $\mathbf{q} \in \mathbb{R}^4$</li>
              <li>불투명도: $\alpha \in \mathbb{R}$</li>
              <li>색 (SH coefficients): $\mathbf{c}$</li>
            </ul>

            <h4>3D Gaussian Grouping</h4>
            <img src="/articles/paper_summaries/gaussian_grouping/img2.png" alt="Framework" />
            <p>
              우선, Gaussian Grouping에서는 각 가우시안이 어떤 object에 속하는지를 표현하기 위해 크기 16의 벡터 identity
              encoding $\mathbf{e} \in \mathbb{R}^{16}$을 파라미터로 추가한다.
            </p>
            <p>
              (a) 단계에서는 2D image와 SAM mask를 모델에 입력한다. (b) 단계에서는 multi-view consistency를 위해 같은
              object를 보고 있는 mask들에 동일한 ID를 부여한다. 이때 zero-shot tracking 모델인 DEVA를 이용해서 frame간
              object의 이동을 추적하고 mask를 associate할 수 있다. 이러한 방식은 각 mask를 별개로 보고 association을
              진행하는 것보다 60배 이상 빠르면서 정확도도 더 높다고 한다.
            </p>
            <p>
              (c) 단계에서는 렌더링을 통한 identity encoding의 학습을 진행한다. Identity encoding은 크기 16의 벡터로, 각
              instance의 ID를 표현하도록 학습된다. 색깔과 같은 방식으로 identity encoding을 2D로 렌더링한 후, 각 mask와
              비교하여 loss를 계산하고 backpropagate하는 방식으로 학습한다.
            </p>
            <p class="math-center">
              $E_{\text{id}} = \sum\limits_{i \in \mathcal{N}} e_i \alpha'_i \prod\limits_{j=1}^{i-1} (1 - \alpha'_j)$
            </p>
            <p>
              Identity encoding을 렌더링하는 구체적인 수식은 위와 같다. $E_{\text{id}}$는 렌더링된 identity encoding,
              $\alpha'_i$는 각 픽셀에서의 Gaussian influence factor에 해당한다. Gaussian influence factor는 아래 수식을
              통해 구한 projection된 covariance $\Sigma^{\text{2D}}$에 가우시안의 opacity $\alpha_i$를 곱하여 구한다.
            </p>
            <p class="math-center">$\Sigma^{\text{2D}} = J W \Sigma^{\text{3D}} W^T J^T$</p>
            <p>
              Loss를 계산하는 과정을 구체적으로 살펴보자. 총 $K$개의 object가 있다고 가정하자. Grouping loss
              $\mathcal{L}_{\text{id}}$는 두 개의 세부 loss 항목 $\mathcal{L}_{\text{2d}}$와 $\mathcal{L}_{\text{3d}}$로
              구성된다.
            </p>
            <p>
              $\mathcal{L}_{\text{2d}}$는 2D identity loss로, 렌더링된 각 픽셀별 identity encoding이 K개의 mask ID 중
              하나로 매핑되도록 한다. 우선 linear layer $f$를 통해 픽셀별 identity encoding을 크기 $K$의 벡터로
              projection한다. 그 후 softmax를 취하여 standard classification을 진행한다. Loss 또한 standard
              cross-entropy loss이다.
            </p>
            <p>
              $\mathcal{L}_{\text{3d}}$은 3D regularization loss로, 공간상 가까이 위치한 가우시안들의 identity
              encoding이 서로 유사해지도록 한다. 이 loss는 occlusion이 많은 상황이나 물체 내부의 가우시안이 더욱 잘
              학습될 수 있도록 한다.
            </p>
            <p class="math-center">
              $\mathcal{L}_{3d} = \cfrac{1}{m} \sum\limits_{j=1}^{m} D_{\text{kl}}(P \| Q) = \cfrac{1}{mk}
              \sum\limits_{j=1}^{m} \sum\limits_{i=1}^{k} F(e_j) \log \left( \cfrac{F(e_j)}{F(e_i')} \right)$
            </p>
            <p>
              위 수식에서 $P$는 현재 계산하고 있는 가우시안의 identity encoding, $Q = \{e'_1, e'_2, \cdots, e'_k\}$는 그
              가우시안의 $k$ nearest neighbors의 identity encoding이다. $F$는 2D identity loss에서도 사용된 linear layer
              $f$와 softmax 연산을 합쳐서 표현한 것이다. $m$은 KL divergence에서의 sampling point 개수이다.
            </p>
            <p>
              3D regularization loss로 인하여 각 가우시안의 identity encoding이 그 주변 $k$개의 가우시안과 유사해지게
              된다.
            </p>
            <p>
              최종 loss는 기존 가우시안의 reconstruction loss $\mathcal{L}_{rec}$과 Grouping loss
              $\mathcal{L}_{\text{id}}$를 합쳐 아래와 같이 표현된다.
            </p>
            <p class="math-center">
              $\mathcal{L}_{\text{render}} = \mathcal{L}_{\text{rec}} + \mathcal{L}_{\text{id}} =
              \mathcal{L}_{\text{rec}} + \lambda_{\text{2d}} \mathcal{L}_{\text{2d}} + \lambda_{\text{3d}}
              \mathcal{L}_{\text{3d}}$
            </p>

            <h4>Gaussian Grouping for Scene Editing</h4>
            <img src="/articles/paper_summaries/gaussian_grouping/img3.png" alt="Gaussian Grouping for Scene Editing" />
            <p>
              위의 과정을 거쳐 grouping이 끝난 후에는, 원하는 방식대로 scene editing을 할 수 있다. Object removal이나
              Scene recomposition (두 물체의 위치 교환)의 경우에는 추가적인 fine tuning 없이 단순히 하나의 group을
              삭제하거나 두 group의 위치를 바꾼다.
            </p>
            <p>
              Object inpainting의 경우에는 우선 해당하는 object의 group을 삭제하고, LaMa 모델의 2D inpainting 결과를
              바탕으로 적은 수의 가우시안을 새로 추가한다. 이후 fine tuning을 진행한다.
            </p>
            <p>
              Object colorization의 경우에는 SH 파라미터만 fine tuning하고, object style transfer의 경우에는 중심점의
              위치와 표준편차까지 fine tuning을 진행한다.
            </p>

            <h2>Experiments</h2>

            <h4>Dataset and Experiment Setup</h4>
            <p>
              Segmentation과 localization 정확도 측정을 위한 데이터셋으로는 LERF-Localization 데이터셋을 변형한
              LERF-Mask 데이터셋을 사용하였다. LERF-Localization에서 3개 scene을 골라 직접 3D annotation을 진행하고 각
              scene에 대해 평균 7.7개의 GT mask label text를 주었다. 그 외 Replica와 ScanNet 데이터셋도 사용하였다.
            </p>
            <p>
              Reconstruction quality 측정을 위한 데이터셋으로는 Mip-NeRF 360, LLFF, Tanks & Temples, Instruct-NeRF2NeRF
              데이터셋을 사용하였다.
            </p>

            <h4>Ablation Experiments</h4>
            <p>
              먼저 Ablation on Mask Cross-view Association 항목에서는 DEVA 모델을 활용하여 mask association을 하는 것의
              효과를 다룬다.
            </p>
          </div>

          <post-footer></post-footer>
        </section>
      </div>
    </main>

    <footer>
      <p>&copy; 2025 Seongmin Jung<br />Designed and developed by Seongmin Jung</p>
    </footer>
  </body>
</html>
