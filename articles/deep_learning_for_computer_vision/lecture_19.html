<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 19: Generative Models I</title>
    <meta name="description" content="Seongmin Jung's personal website" />
    <meta name="keywords" content="Seongmin Jung, Robotics" />
    <meta name="author" content="Seongmin Jung" />
    <meta name="language" content="English" />
    <link rel="shortcut icon" href="/favicon.png" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LL44K1WZ0G"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-LL44K1WZ0G");
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

    <!-- Load an icon library to show a hamburger menu (bars) on small screens -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <!-- import css -->
    <link rel="stylesheet" href="/css/index.css" />
    <!-- import js -->
    <script src="/js/script.js" defer></script>
  </head>

  <body>
    <header>
      <img class="cover" src="/asset/cover.jpg" alt="Seongmin Jung" />
      <h1 class="title"><a href="/index.html">Seongmin Jung</a></h1>
      <nav class="nav">
        <a href="/index.html">Home</a>
        <a href="/projects.html">Projects</a>
        <a href="/articles.html" class="bold">Articles</a>
        <a href="/blog.html">Blog</a>
        <i id="nav-toggle" class="nav-toggle fa fa-bars" onclick="toggleNav()"></i>
      </nav>
    </header>

    <div id="nav-modal-bg" onclick="toggleNav()"></div>
    <nav id="nav-modal">
      <a href="/index.html">Home</a>
      <a href="/projects.html">Projects</a>
      <a href="/articles.html" class="bold">Articles</a>
      <a href="/blog.html">Blog</a>
    </nav>

    <main>
      <div class="container">
        <section id="post">
          <a id="category-tag" href="/articles/deep_learning_for_computer_vision.html"
            ><i class="fa fa-book"></i> Deep Learning for Computer Vision</a
          >
          <h1>Lecture 19: Generative Models I</h1>
          <p class="date">Posted on <time datetime="2024-08-29">August 29, 2024</time></p>
          <div class="post-body">
            <h2>Supervised vs Unsupervised Learning</h2>
            <p>
              지금까지 우리는 Supervised Learning을 다루었다. Supervised Learning에서는 training dataset이 input과
              output의 쌍 $(X, Y)$으로 주어진다. 따라서, 모델은 input $X$를 output $Y$로 매핑하는 함수를 학습하게 된다.
            </p>
            <p>
              반면, Unsupervised Learning에서는 output이 주어지지 않는다. 대신 모델은 input 데이터에 내재된 패턴을
              학습한다. Unsupervised Learning의 예시로는 clustering, dimensionality reduction, feature learning, density
              estimation 등이 있다.
            </p>

            <h2>Discriminative vs Generative Models</h2>
            <p>
              AI 모델을 구분할 수 있는 또 다른 기준으로는 Discriminative vs Generative가 있다. Discriminative Model은
              $P(Y|X)$를 학습한다. 즉, input $X$가 주어졌을 때 output $Y$의 확률 분포를 학습하는 것이다.
              Classification을 예시로 들면, 이미지가 주어졌을 때 class를 예측하는 것이다. 이러한 형태의 모델에서는
              Output $Y$의 확률 분포를 모두 합하면 1이 되어야 하기 때문에, 한 class에 대한 확률이 높아지면 다른 class에
              대한 확률은 자연스럽게 낮아진다는 특징이 있다.
            </p>
            <p>
              반면, Generative Model은 $P(X)$를 학습한다. 즉, clustering과 같이 학습 데이터 $X$를 보고 $X$의 확률 분포를
              학습하는 것이다. 따라서 이 확률 분포를 이용하면 새로운 데이터를 생성할 수도 있고, 특정 input image가
              outlier인지 여부도 판단할 수 있다. 또한 label 없이도 이미지의 특징을 학습하여 feature를 추출할 수도 있다.
            </p>
            <p>
              Conditional Generative Model은 $P(X|Y)$를 학습한다. 즉, class $Y$가 주어졌을 때 image $X$의 확률 분포를
              추정하는 것이다. 따라서 이 분포를 이용하면 주어진 class에 해당하는 이미지를 생성할 수 있다.
            </p>
            <img
              src="/articles/deep_learning_for_computer_vision/lecture_19/img1.png"
              alt="Taxonomy of Generative Models"
            />
            <p>
              Generative Model은 위 그림과 같이 분류할 수 있다. 생성된 확률 분포를 직접 얻을 수 있는지 여부에 따라
              explicit과 implicit density model로 나뉜다. Explicit은 또다시 정확한 확률 분포를 얻을 수 있는지, 추정치만
              얻을 수 있는지에 따라 tractable과 approximate density model로 나뉜다. 이 강의에서는 Autoregressive Model,
              Variational Autoencoder, Generative Adversarial Network (GAN)에 대해 다룬다.
            </p>

            <h2>Autoregressive Models</h2>
            <p>
              Explicit density model의 목표는 $X$에 대한 확률 분포 $p(x) = f(x, W)$를 직접 얻는 것이다. 이러한 모델을
              훈련시키기 위해 Maximum Likelihood Estimation (MLE)을 사용한다. 즉, 지금까지 입력된 training dataset에
              대한 확률 분포를 최대화하는 $W$를 찾는 것이다. 주어진 dataset을 $x^{(1)}, x^{(2)}, \cdots, x^{(N)}$이라고
              할 때, MLE는 다음과 같이 정의된다.
            </p>
            <p class="math-center">$W^* = \arg \max_W \prod_{i=1}^N p(x^{(i)})$</p>
            <p>계산의 편의를 위해 log를 취하면 다음과 같이 정의된다. 이때 $\arg \max$의 결과는 바뀌지 않는다.</p>
            <p class="math-center">$W^* = \arg \max_W \sum_{i=1}^N \log p(x^{(i)})$</p>
            <p>이때 $p(x^{(i)})$를 $f(x^{(i)}, W)$로 근사하면 다음과 같다.</p>
            <p class="math-center">$W^* = \arg \max_W \sum_{i=1}^N \log f(x^{(i)}, W)$</p>
            <p>
              위의 수식을 loss function으로 사용해서 모델을 학습시키면, 모델은 training dataset에 대한 확률 분포를
              근사하게 된다. 이러한 셋업은 Explicit density model들의 공통된 특징이다.
            </p>
            <p>
              그렇다면 Autoregressive Model에서 함수 $f$를 정의하는 방법을 알아보자. 우선 입력 $x$가 $x = (x_1, x_2,
              \cdots, x_T)$ 등 $T$개의 부분으로 나뉘어져 있다고 가정한다. 이미지에서는 pixel에 해당한다. 그러면
              확률분포를 아래와 같이 나타낼 수 있다.
            </p>
            <p class="math-center">
              $\begin{align*} p(x) & = p(x1, x2, \cdots, x_T) \\ & = p(x_1) p(x_2|x_1) p(x_3|x_1, x_2) \cdots \\ & =
              \prod_{t=1}^T p(x_t|x_1, x_2, \cdots, x_{t-1}) \end{align*}$
            </p>
            <p>
              위의 수식을 잘 살펴보면 RNN과 형태가 유사함을 알 수 있다. 즉, 지금까지 생성된 부분 전체를 이용하여 다음
              부분에 해당하는 확률 분포를 계산하는 것이다. 이러한 방식을 통해 모든 부분에 해당하는 확률 분포를 차례차례
              구할 수 있다.
            </p>
            <p>
              이러한 Autoregressive Model의 장점으로는 단순한 구조와 loss function을 가지기 때문에 evaluation하기 쉽다는
              것이 있다. 단순히 test set image들이 예측된 확률분포에 잘 들어맞는지를 비교하면 된다. 하지만 generation이
              느리다는 단점이 있다.
            </p>

            <h3>PixelRNN</h3>
            <img class="half" src="/articles/deep_learning_for_computer_vision/lecture_19/img2.png" alt="PixelRNN" />
            <p>
              Autoregressive Model의 한 예시로 PixelRNN이 있다. 이는 위의 Autoregressive Model를 이미지 픽셀에
              적용하는데, 1차원이 아닌 2차원 RNN의 형태이다. 즉, 한 픽셀의 hidden state는 위쪽 픽셀과 왼쪽 픽셀 모두에
              dependent하다.
            </p>
            <p class="math-center">$h_{x,y} = f(h_{x-1,y}, h_{x,y-1}, W)$</p>
            <p>
              최종적으로 각 픽셀별 hidden state를 이용하여 그 픽셀의 값이 0부터 255까지의 값이 될 확률을 계산하고, 가장
              높은 확률을 가지는 값을 선택하여 이미지를 생성한다.
            </p>
            <p>
              이러한 방식은 모든 픽셀을 순차적으로 처리해야 하기 때문에 training과 testing 모두에서 속도가 매우 느리다는
              단점이 있다.
            </p>

            <h3>PixelCNN</h3>
            <img class="half" src="/articles/deep_learning_for_computer_vision/lecture_19/img3.png" alt="PixelCNN" />
            <p>
              위의 PixelRNN의 단점을 해결하기 위한 모델이 PixelCNN이다. CNN 방식을 사용하기 때문에 병렬 처리가 가능하다.
              따라서 training은 PixelRNN보다 빠르지만, 여전히 testing은 순차적으로 진행해야 하므로 느리다.
            </p>

            <h2>Variational Autoencoder (VAE)</h2>
            <p>
              Variational Autoencoder는 Implicit density model로, 확률 분포를 직접 얻을 수 없다. 대신, 여기서는 확률
              분포의 최솟값을 최대화하는 방식으로 학습한다.
            </p>

            <h3>Autoencoder</h3>
            <img class="half" src="/articles/deep_learning_for_computer_vision/lecture_19/img4.png" alt="Autoencoder" />
            <p>
              VAE에 대해 살펴보기 전에 일반적인 Autoencoder의 구조에 대해 먼저 살펴보자. Autoencoder는 raw data
              $x$로부터 feature vector $z$를 추출하는 unsupervised learning 방법이다. 즉 Ground Truth $z$가 존재하지
              않는다. Feature vector $z$는 $x$를 더욱 간결하고 고차원적으로 표현하는 feature들을 가지고 있다. 이렇게
              추출된 feature vector $z$는 transfer learning을 통해 다양한 task에 사용될 수 있다.
            </p>
            <p>
              이렇게 입력 $x$를 feature vector $z$로 변환하는 네트워크를 encoder라고 한다. Encoder는 FC layer, CNN 등
              다양한 형태로 구성할 수 있다. 이러한 $z$를 <b>Latent Vector</b>라고도 한다.
            </p>
            <p>
              이러한 Encoder를 실제로 학습시키기 위해서는 Decoder가 필요하다. Decoder는 feature vector $z$를 입력받아
              원래의 입력 $x$를 출력하도록 학습하는 모델이다. 이렇게 되면, 원래 입력 $x$와 decoder의 출력 $\hat{x}$
              사이의 L2 loss를 최소화하는 방향으로 encoder와 decoder를 모두 학습시킬 수 있다. 이러한 방식을 통해 GT data
              없이도 $x$의 특징을 잘 담고 있는 feature vector $z$를 추출할 수 있다.
            </p>
            <p>
              학습을 마친 후에는 encoder만을 사용하게 된다. Encoder로부터 추출된 feature vector $z$를 직접 다양한 task에
              사용할 수 있다. 하지만 이러한 일반적인 Autoencoder는 probabilistic하지 않기 때문에 새로운 데이터를
              sampling할 수는 없다는 문제가 있다.
            </p>

            <h3>Variational Autoencoder (VAE)</h3>
            <p>
              Variational Autoencoder에서는 사용자가 latent vector $z$에 직접 접근할 수 없다. 따라서 $z$로부터 $x$를
              sampling하는 과정은 아래와 같이 이루어진다.
            </p>
            <img class="half" src="/articles/deep_learning_for_computer_vision/lecture_19/img5.png" alt="VAE" />
            <p>
              우선 $z$의 prior distribution $p_{\theta^*}(z)$으로부터 $z$를 sampling한다. 이때 prior distribution은 보통
              Gaussian distribution으로 가정한다. 이 분포는 학습되는 것이 아닌 사전에 정해지는 것이다.
            </p>
            <p>
              이후 $z$에 대한 conditional distribution $p_{\theta^*}(x|z^{(i)})$로부터 $x$를 sampling한다. 이때
              $p_{\theta^*}(x|z^{(i)})$는 사전에 정해지는 것이 아니라 뉴럴 네트워크를 통해 학습된다. Autoencoder에서의
              decoder 부분과 유사하다고 볼 수 있다. 이러한 과정을 통해 $x$를 생성할 수 있다.
            </p>
          </div>
          <div class="post-footer">
            <div class="post-footer-profile">
              <img src="/asset/cover.jpg" alt="Seongmin Jung" />
              <h1>Seongmin Jung</h1>
            </div>
            <h2>
              Other posts in
              <a href="/articles/deep_learning_for_computer_vision.html">Deep Learning for Computer Vision</a> category
            </h2>
            <ul>
              <li>
                <a href="/articles/deep_learning_for_computer_vision/lecture_19.html">
                  <span><b>Lecture 19: Generative Models I</b></span>
                  <time datetime="2024-08-29">August 29, 2024</time>
                </a>
              </li>
              <li>
                <a href="/articles/deep_learning_for_computer_vision/lecture_18.html">
                  <span>Lecture 18: Vision Transformers</span>
                  <time datetime="2024-08-24">August 24, 2024</time>
                </a>
              </li>
              <li>
                <a href="/articles/deep_learning_for_computer_vision/lecture_17.html">
                  <span>Lecture 17: Attention</span>
                  <time datetime="2024-06-15">June 15, 2024</time>
                </a>
              </li>
              <li>
                <a href="/articles/deep_learning_for_computer_vision/lecture_16.html">
                  <span>Lecture 16: Recurrent Networks</span>
                  <time datetime="2024-06-04">June 4, 2024</time>
                </a>
              </li>
              <li>
                <a href="/articles/deep_learning_for_computer_vision/lecture_15.html">
                  <span>Lecture 15: Image Segmentation</span>
                  <time datetime="2024-05-30">May 30, 2024</time>
                </a>
              </li>
            </ul>
          </div>
        </section>
      </div>
    </main>

    <footer>
      <p>&copy; 2024 Seongmin Jung<br />Designed and developed by Seongmin Jung</p>
    </footer>
  </body>
</html>
