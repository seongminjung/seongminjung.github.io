<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 3: Linear Classifiers</title>
    <meta name="description" content="Seongmin Jung's personal website" />
    <meta name="keywords" content="Seongmin Jung, Robotics" />
    <meta name="author" content="Seongmin Jung" />
    <meta name="language" content="English" />
    <link rel="shortcut icon" href="/favicon.png" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LL44K1WZ0G"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-LL44K1WZ0G");
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

    <!-- Load an icon library to show a hamburger menu (bars) on small screens -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <!-- import css -->
    <link rel="stylesheet" href="/css/index.css" />
    <!-- import js -->
    <script src="/js/script.js" defer></script>
  </head>

  <body>
    <header>
      <img class="cover" src="/asset/cover.jpg" alt="Seongmin Jung" />
      <h1 class="title"><a href="/index.html">Seongmin Jung</a></h1>
      <nav class="nav">
        <a href="/index.html">Home</a>
        <a href="/projects.html">Projects</a>
        <a href="/study.html" class="bold">Study</a>
        <a href="/blog.html">Blog</a>
        <i id="nav-toggle" class="nav-toggle fa fa-bars" onclick="toggleNav()"></i>
      </nav>
    </header>

    <div id="nav-modal-bg" onclick="toggleNav()"></div>
    <nav id="nav-modal">
      <a href="/index.html">Home</a>
      <a href="/projects.html">Projects</a>
      <a href="/study.html" class="bold">Study</a>
      <a href="/blog.html">Blog</a>
    </nav>

    <main>
      <div class="container">
        <section id="post">
          <a id="series-tag" href="/study/deep_learning_for_computer_vision.html"
            ><i class="fa fa-book"></i> Deep Learning for Computer Vision</a
          >
          <h1>Lecture 3: Linear Classifiers</h1>
          <p class="date">Posted on <time datetime="2024-03-26">March 26, 2024</time></p>
          <div class="post-body">
            <h2>Linear Classifier</h2>
            <img src="/study/deep_learning_for_computer_vision/lecture_3/img1.png" alt="Linear classifier equation" />
            <p>
              kNN에서는 단순히 이미지 자체를 비교해서 분류를 수행하였다. 즉, 데이터셋 자체가 분류 모델이었다. 하지만
              앞으로 살펴볼 'parametric model'에서는 데이터셋에 대한 정보를 함축하여, 데이터셋 전체를 기억하지 않고도
              분류를 더욱 효율적으로 수행할 수 있다.
            </p>
            <p class="math-center">$\mathbf{y} = f(\mathbf{x}, \mathbf{W})$</p>
            <p>
              위 수식에서 $\mathbf{x}$는 입력된 픽셀값, $\mathbf{W}$는 데이터셋에 대한 정보가 함축된 'weight' 행렬,
              $f$는 $\mathbf{x}$와 $\mathbf{W}$를 이용하여 입력 이미지가 각 클래스에 속할 가능성을 클래스별 점수로
              출력하는 함수이다. 이 점수를 통해 가장 높은 점수를 가진 클래스를 선택할 수 있고, 우리는 데이터셋 전체를
              기억할 필요 없이 올바른 $\mathbf{W}$만을 갖고 있으면 쉽고 빠르게 분류를 수행할 수 있다.
            </p>
            <p>
              올바른 $\mathbf{W}$를 찾는 법은 아래쪽에서 살펴보기로 하고, 우선 $\mathbf{W}$가 있을 때 $\mathbf{x}$와
              $\mathbf{W}$를 어떻게 연산해야 할지 살펴보자. 가장 간단한 방법은 아래와 같이 단순히 픽셀값과 그에 해당하는
              가중치를 곱한 후 모두 더하는 것이다. 사칙연산만으로 이루어져 있기 때문에 linear classifier라는 명칭이
              붙었다. 이처럼 단순한 linear classifier는 대부분의 딥러닝 모델의 가장 기본적인 building block이 된다.
            </p>
            <p class="math-center">$f(\mathbf{x}, \mathbf{W}) = \mathbf{W} \cdot \mathbf{x} + \mathbf{b}$</p>
            <img src="/study/deep_learning_for_computer_vision/lecture_3/img2.png" alt="Linear classifier visualized" />
            <p>
              위 그림은 2x2 크기의 이미지를 3개의 클래스 중 하나로 분류하는 과정을 나타낸 것이다. 먼저 2x2 픽셀인
              이미지를 4x1의 1차원 벡터 $\mathbf{x}$로 펼친다. 이후 3x4의 weight matrix $\mathbf{W}$와 입력 벡터
              $\mathbf{x}$를 곱한 후 3x1의 bias 벡터 $\mathbf{b}$를 더하여 3x1의 점수 벡터를 만든다. 이 점수 벡터의 각
              원소는 입력 이미지가 해당 클래스에 속할 가능성을 나타낸다.
            </p>
            <p>
              <span class="color-red"> 이때, linear classification을 <b>template matching</b>으로 이해할 수 있다!</span>
            </p>
            <p>
              무슨 말인가 하면, $\mathbf{W}$의 각 row는 입력 이미지와 곱해져 하나의 클래스에 대한 점수를 만들어낸다. 첫
              번째 row는 cat, 두 번째 row는 dog, 세 번째 row는 ship에 대한 점수를 만들어내는 식이다. 그렇다면 각 row가
              어떨 때 점수가 가장 높을까?
            </p>
            <img src="/study/deep_learning_for_computer_vision/lecture_3/img3.jpg" alt="Template matching" />
            <p>
              우선 첫 번째 경우를 보면, $\mathbf{W}$의 한 row와 $\mathbf{x}$의 부호가 항상 반대이기 때문에 곱은 항상
              음수가 나온다. 따라서 쌍을 이루는 원소들의 부호가 같아야 점수가 높아짐을 알 수 있다. 두 번째 경우를 보면,
              $\mathbf{W}$와 $\mathbf{x}$의 그래프 형태가 유사하지 않다. 이 경우에는 예를 들어 $\mathbf{x}$의 마지막
              원소가 양수가 된다면 점수가 더 높아질 수 있다. 즉, 고양이에 해당하는 row는 고양이 사진이 들어왔을 때만
              높은 점수를 출력하고 다른 사진이 들어오면 낮은 점수를 출력해야 하는데 다른 클래스에 더 높은 점수를 줄 수도
              있는 것이다. 세 번째 경우를 보면, $\mathbf{W}$와 $\mathbf{x}$의 형태가 거의 일치한다. 이 경우 고양이
              사진에만 높은 점수를 출력하는 이상적인 $\mathbf{W}$가 된다.
            </p>
            <p>
              우리는 수많은 training 데이터셋으로 학습하므로, 결과적으로 $\mathbf{W}$의 각 row는 각 클래스에 해당하는
              이미지들을 한데 모아놓은 것 같은 형태가 된다. 실제로 CIFAR-10 데이터셋으로 학습시킨 $\mathbf{W}$의 각
              row를 이미지로 변환해보면 아래 그림과 같다.
            </p>
            <img src="/study/deep_learning_for_computer_vision/lecture_3/img4.png" alt="W visualization" />
            <p>
              car 클래스의 경우 자동차의 정면 형상이 보이며 deer 클래스의 경우 연두색 배경에 갈색 물체가 있는 등 각
              클래스에 해당하는 물체를 뭉뚱그려 놓은 듯한 형체가 보인다. 이때 중요한 것은 horse 클래스의 경우 말의
              머리가 양쪽에 달려 있는 것처럼 보이는데, 실제로 말의 머리가 양쪽에 달려 있는 이미지는 없다. 즉, linear
              classifier의 경우 한 클래스당 하나의 템플릿밖에 가지지 못하기 때문에 그 물체가 보여질 수 있는 모든 경우를
              뭉뚱그려 놓은 형태의 템플릿이 된다.
            </p>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_3/img5.png"
              alt="Decision boundary"
            />
            <p>
              Linear classifier에 대한 또 다른 해석으로는 decision boundary가 있다. 대부분의 인공지능 교재에 많이 보이는
              그림인데, 이미지들을 고차원 평면상에 놓고 그 사이에 구분선을 그어 한 클래스와 다른 클래스들을 구분하는
              것이다. 즉, $f(\mathbf{x}, \mathbf{W})$의 결과가 특정 값보다 크면 그 클래스에 포함, 아니면 포함하지 않는
              것으로 보는 것이다. 이러한 해석을 통해 linear classification의 한계점을 파악할 수 있다.
            </p>
            <img
              src="/study/deep_learning_for_computer_vision/lecture_3/img6.png"
              alt="Decision boundary visualization"
            />
            <p>
              위 그림을 보면, 두 클래스의 하나의 직선으로 완전히 구분하는 것이 불가능하다. 왼쪽 경우처럼 픽셀의 수뿐만
              아니라 이미지 내 사람의 수를 홀수와 짝수로 분류하는 문제 등은 linear classifier로 해결하기 어렵다. 오른쪽
              경우처럼 한 클래스가 여러 곳에 퍼져 있는 경우도 있을 수 있는데, 위 horse 클래스에서 말이 왼쪽 또는 오른쪽
              둘 중 하나만 보고 있는 경우가 이에 해당된다. 이 또한 두 경우를 한꺼번에 다른 클래스와 구분짓는 것은
              어렵다. 결국 이 문제는 linear classifier를 다층으로 쌓아서 해결할 수 있는데, 이는 추후 강의에서 살펴볼
              예정이다.
            </p>

            <h2>Loss Function</h2>
            <p>
              그렇다면 올바른 $\mathbf{W}$를 어떻게 찾을 수 있을까? 이를 위해서는 $\mathbf{W}$가 얼마나 좋고 나쁜지를
              수치화해야 한다. 이를 loss function이라고 한다.
            </p>

            <h2>Summary</h2>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_3/img7.png"
              alt="Linear classifier summary"
            />
            <p>
              이번 강의에서는 linear classifier가 어떻게 작동하는지, $\mathbf{W}$는 어떤 의미를 가지고 있는지
              살펴보았다. 다음 강의에서는 loss function과 optimization을 통해 어떻게 최적의 $\mathbf{W}$를 찾을 수
              있는지 알아본다.
            </p>
          </div>

          <div class="post-footer">
            <div class="post-footer-profile">
              <img src="/asset/cover.jpg" alt="Seongmin Jung" />
              <h1>Seongmin Jung</h1>
            </div>
            <h2>
              Other posts in
              <a href="/study/deep_learning_for_computer_vision.html">Deep Learning for Computer Vision</a> series
            </h2>
            <ul>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_3.html">
                  <span><b>Lecture 3: Linear Classifiers</b></span>
                  <time datetime="2024-03-28">March 28, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_2.html">
                  <span>Lecture 2: Image Classification</span>
                  <time datetime="2024-03-26">March 26, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_1.html">
                  <span>Lecture 1: Introduction to Deep Learning for Computer Vision</span>
                  <time datetime="2024-03-26">March 26, 2024</time>
                </a>
              </li>
            </ul>
          </div>
        </section>
      </div>
    </main>

    <footer>
      <p>&copy; 2024 Seongmin Jung<br />Designed and developed by Seongmin Jung</p>
    </footer>
  </body>
</html>
