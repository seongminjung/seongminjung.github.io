<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 14: Object Detectors</title>
    <meta name="description" content="Seongmin Jung's personal website" />
    <meta name="keywords" content="Seongmin Jung, Robotics" />
    <meta name="author" content="Seongmin Jung" />
    <meta name="language" content="English" />
    <link rel="shortcut icon" href="/favicon.png" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LL44K1WZ0G"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-LL44K1WZ0G");
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

    <!-- Load an icon library to show a hamburger menu (bars) on small screens -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <!-- import css -->
    <link rel="stylesheet" href="/css/index.css" />
    <!-- import js -->
    <script src="/js/script.js" defer></script>
  </head>

  <body>
    <header>
      <img class="cover" src="/asset/cover.jpg" alt="Seongmin Jung" />
      <h1 class="title"><a href="/index.html">Seongmin Jung</a></h1>
      <nav class="nav">
        <a href="/index.html">Home</a>
        <a href="/projects.html">Projects</a>
        <a href="/study.html" class="bold">Study</a>
        <a href="/blog.html">Blog</a>
        <i id="nav-toggle" class="nav-toggle fa fa-bars" onclick="toggleNav()"></i>
      </nav>
    </header>

    <div id="nav-modal-bg" onclick="toggleNav()"></div>
    <nav id="nav-modal">
      <a href="/index.html">Home</a>
      <a href="/projects.html">Projects</a>
      <a href="/study.html" class="bold">Study</a>
      <a href="/blog.html">Blog</a>
    </nav>

    <main>
      <div class="container">
        <section id="post">
          <a id="series-tag" href="/study/deep_learning_for_computer_vision.html"
            ><i class="fa fa-book"></i> Deep Learning for Computer Vision</a
          >
          <h1>Lecture 14: Object Detectors</h1>
          <p class="date">Posted on <time datetime="2024-05-19">May 19, 2024</time></p>
          <div class="post-body">
            <h2>Fast R-CNN</h2>
            <p>
              지난 강의에서는 R-CNN에 대해 알아보았다. 하지만 R-CNN 모델은 한 이미지당 2000개의 영역에 대해 각각 CNN을
              실행해야 하므로 training과 test 모두에서 매우 느리다는 단점이 있다. 이때 CNN에서 가장 연산량이 많은 부분이
              초반 몇 stage이므로, 이 점을 개선해 속도를 높인 Fast R-CNN이 등장하게 된다.
            </p>
            <img class="half" src="/study/deep_learning_for_computer_vision/lecture_14/img1.png" alt="Fast R-CNN" />
            <p>
              Fast R-CNN은 이미지 전체에 대해 CNN을 한 번만 실행하고, 추출된 feature map 상에서 RoI(Regions of
              Interest)를 구하는 방식을 사용한다. 연산량이 많은 초반 conv layer 단계들은 첫 단계에 포함하고, 이후 각
              영역에 대해 fully connected layer만을 사용하거나 마지막 stage만을 사용하여 각 영역에 대한 class score와
              bounding box를 예측한다. 이 FC layer는 R-CNN에서 이미지 영역 자체를 입력받는 모델에 비해 훨씬 가볍고
              빠르게 동작한다.
            </p>
            <p>
              이때, region proposal은 여전히 RGB 이미지를 이용하는 고전적인 방식을 사용한다. Fast R-CNN은 feature map
              상에서 영역을 crop해야 하는데, region proposal을 feature map에 어떻게 대응시킬 수 있을까?
            </p>

            <h3>Projecting Bounding Boxes</h3>
            <img src="/study/deep_learning_for_computer_vision/lecture_14/img2.png" alt="Projecting points" />
            <p>
              위 사진과 같이 output image의 특정 점에 대해 input image에서의 receptive field를 얻을 수 있다. input과
              output 사이에 어떤 layer가 있든, 이미지의 크기가 유지된다면 receptive field의 중심은 항상 output image의
              점과 같은 위치에 있다.
            </p>
            <img
              src="/study/deep_learning_for_computer_vision/lecture_14/img3.png"
              alt="Projecting points with different image size"
            />
            <p>
              이미지의 크기가 달라진다면, 좌표의 절대적인 값은 달라지겠지만 output image의 특정 점에 대해 input
              image상에 대응되는 한 점은 항상 특정할 수 있을 것이다.
            </p>
            <img class="half" src="/study/deep_learning_for_computer_vision/lecture_14/img4.png" alt="RoI pooling" />
            <p>
              이를 직사각형의 모든 꼭짓점에 대해 적용하면 RGB 이미지 위의 region proposal을 feature map 상으로 그대로
              옮길 수 있다. RoI pooling이 된다. 이때, Region proposal을 통해 얻은 영역은 그 크기가 제각각일 것이다.
              CNN은 정해진 크기의 이미지만을 입력으로 받을 수 있으므로 R-CNN에서는 모든 영역을 224x224로 resize하였다.
              이와 유사하게 Fast R-CNN에서도 crop된 feature map을 고정된 크기로 resize하여 사용한다. 보통 7x7이나
              14x14로 resize한다. 이때 사용되는 방법이 RoI pooling과 RoI aligning이다.
            </p>

            <h3>RoI Pooling</h3>
            <img class="half" src="/study/deep_learning_for_computer_vision/lecture_14/img5.png" alt="RoI pooling" />
            <p>
              RoI pooling은 가장 먼저 RoI의 꼭짓점 좌표를 정수로 반올림하여 격자에 딱 맞게 한다. 이미지의 resolution이
              달라질 경우 bounding box를 feature map을 옮겼을 때 좌표값이 소수점이 될 수 있기 때문이다. 이후 단순히
              직사각형을 최대한 균일하게 7x7 또는 14x14의 영역으로 분할하고, 각 영역의 최댓값을 구하는 max pooling을
              적용한다. 이 방법을 통해 어떤 크기나 비율을 가진 직사각형이라도 일정한 크기로 resize할 수 있다.
            </p>
            <p>하지만 RoI pooling에는 두 가지의 문제점이 있다.</p>
            <ul>
              <li>꼭짓점의 좌표를 반올림하여 강제로 격자에 맞추기 때문에, 이 과정 자체가 오차를 발생시키게 된다.</li>
              <li>
                7x7이나 14x14개의 영역으로 분할하는 과정에서 각 변의 픽셀 수가 정확히 나누어떨어지지 않으면 위 예시와
                같이 균일하지 않게 분할된다.
              </li>
            </ul>

            <h3>RoI Aligning</h3>
            <img class="half" src="/study/deep_learning_for_computer_vision/lecture_14/img6.png" alt="RoI aligning" />
            <p>
              RoI aligning은 RoI pooling의 문제점을 해결하기 위해 등장한 방법이다. RoI aligning은 소수점 좌표를
              반올림하는 대신 bilinear interpolation을 이용하여 위 두 문제를 동시에 해결하였다.
            </p>
            <p>
              먼저 직사각형을 위와 동일하게 7x7 또는 14x14로 분할한다. 좌표값이 정수가 되지 않아도 되기 때문에 완벽하게
              동일한 크기로 분할할 수 있다. 이후 각 영역의 대표값을 구해야 하는데, 픽셀값 자체를 이용하는 max pooling이
              아니라 먼저 각 영역에 대해 고르게 샘플 점들을 분포시킨다. 위 그림은 RoI를 균일하게 4개의 영역을 나누고 각
              영역에 대해 4개의 샘플 점을 배치한 것이다.
            </p>
            <p>
              이때 이 샘플 점들 또한 정수 좌표값을 갖지 않을 수 있다. 따라서 각 샘플 점에 대해 bilinear interpolation을
              이용하여 각 점에 대한 값을 구하고, 이 샘플 점들에 대해 max pooling을 적용하여 최종적으로 각 영역의
              대표값을 구한다. Bilinear interpolation의 구체적인 과정은 아래와 같다.
            </p>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_14/img7.png"
              alt="Bilinear interpolation"
            />
            <p>
              위와 같이 점 $(6.5, 5.8)$에서의 값을 구하고자 할 때, 먼저 가장 가까운 네 점 $(6, 5), (6, 6), (7, 5), (7,
              6)$의 값을 구한다. 이후 각 점에 대해 거리에 반비례하는 가중치를 부여하여 값을 계산한다.
            </p>
            <p class="math-center">
              $f_{6.5, 5.8} = (f_{6, 5} \cdot 0.5 * 0.2) + (f_{7, 5} \cdot 0.5 * 0.2) + (f_{6, 6} \cdot 0.5 * 0.8) +
              (f_{7, 6} \cdot 0.5 * 0.8)$
            </p>
            <p>이를 일반적인 수식으로 나타내면 아래와 같다.</p>
            <p class="math-center">
              $f_{x, y} = \sum_{i, j=1}^{2} f_{x_i, y_j} \cdot \max(0, 1 - |x - x_i|) \cdot \max(0, 1 - |y - y_j|)$
            </p>
            <p>이러한 RoI aligning 방식은 RoI pooling보다 훨씬 더 흔하게 사용된다.</p>
            <img src="/study/deep_learning_for_computer_vision/lecture_14/img8.png" alt="Fast R-CNN results" />
            <p>
              Fast R-CNN과 기존 R-CNN의 training/test time을 비교하면 위와 같다. Fast R-CNN은 R-CNN에 비해 학습 시에는
              약 10배, test 시에는 약 20배 빠른 것을 알 수 있다. 또 하나의 주목할 점으로는, Fast R-CNN에서는
              bottleneck이 더 이상 conv layer가 아니라 전통적인 방식의 region proposal이 되었다는 점이다.
            </p>

            <h2>Faster R-CNN</h2>
            <img class="half" src="/study/deep_learning_for_computer_vision/lecture_14/img9.png" alt="Faster R-CNN" />
            <p>
              앞서 언급한 Fast R-CNN의 문제점을 해결하기 위해 region proposal을 기존의 알고리즘 대신 CNN 모델로 대체하여
              그 속도를 다시 한 번 높인 것이 Faster R-CNN이다. 이를 Region Proposal Network(RPN)이 담당한다. 기존 Fast
              R-CNN에서도 사용되었던 backbone network에서 image feature map가 나오면, 이를 RPN에 입력으로 넣어 region
              proposal을 얻는다. 이미 사용되고 있는 모델의 출력을 이용하기 때문에 추가적인 연산량이 거의 발생하지
              않는다. 최종적으로 RPN의 출력으로 나온 RoI에 대해 Fast R-CNN과 동일한 방식으로 classification과 bounding
              box regression을 수행한다.
            </p>

            <h3>Region Proposal Network (RPN)</h3>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_14/img10.png"
              alt="Region Proposal Network (RPN)"
            />
            <p>
              RPN은 위의 오른쪽 이미지에 해당하는 feature map을 입력으로 받는다. 이 Feature map의 각 픽셀에 대해 원본
              이미지에 대응되는 점을 구하고, 이 점을 중심으로 특정 크기의 bounding box를 그린다. 이를 anchor box라고
              한다. RPN은 각 anchor box가 물체를 잘 포함하는지 아닌지에 따라 대응되는 feature map 상 픽셀을 positive와
              negative로 binary classification한다. GT box는 RGB 이미지를 기준으로 제공되기 때문에 이러한 우회적인
              방법을 사용하는 것이다. 이때 anchor box의 크기는 hyperparameter이다.
            </p>
            <img class="half" src="/study/deep_learning_for_computer_vision/lecture_14/img11.png" alt="RPN i/o" />
            <p>
              이를 위해 RPN은 conv layer로 구성되며 위 그림에서 입력이 512x5x6이라면 출력은 2x5x6이 된다. 각 픽셀이
              positive일지 negative일지를 binary classification하였기 때문에 이러한 shape가 된다. 한 Anchor box와 어떤
              GT box 사이의 IoU가 0.7보다 크면 그 anchor box는 positive로 분류되고, 모든 GT box와 가장 IoU가 높은 anchor
              box 또한 positive로 분류된다. IoU가 0.3보다 작은 anchor box는 negative로 분류된다.
            </p>
            <p>
              Classification뿐만 아니라, RPN 네트워크를 이용해 bounding box regression까지 수행한다. 각 anchor box에
              대해 GT box와의 차이를 regression하여 bounding box를 조정하는 것이다. 이때, anchor box와 GT box의
              transform에 해당하는 $t_x, t_y, t_w, t_h$를 구하는 것이 목표이다. 따라서 classification과 별개의 branch를
              구성하여 transform을 구한다. 따라서 output shape는 4x5x6이 된다.
            </p>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_14/img12.png"
              alt="Multiple anchor boxes"
            />
            <p>
              또한, 실제로는 각 점당 크기와 비율이 다른 k개의 anchor box를 동시에 사용한다. 이에 따라 classification
              output은 2kx5x6, transform output은 4kx5x6이 된다. 이때, k는 hyperparameter이다.
            </p>
            <p>
              Test 시에는, 각 anchor box에 대해 classification score와 transform을 구하고 transform를 이용해 bounding
              box regression을 수행한다. 이후 NMS를 적용하여 중복을 제거하고, 남은 bounding box에 대해 positive일 확률이
              높은 상위 300개를 선택하여 최종적인 region proposal을 얻는다.
            </p>
            <p>
              정리하면, Faster R-CNN은 총 4개의 Loss를 사용한다. 먼저 RPN의 classification loss와 transform loss, 그리고
              per-region network의 classification loss와 transform loss이다. 이를 jointly training하여 최종적으로 Faster
              R-CNN을 학습시킨다.
            </p>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_14/img13.png"
              alt="Faster R-CNN results"
            />
            <p>
              Faster R-CNN은 Fast R-CNN에 비해 test 시 약 10배 빠른 것을 알 수 있다. 그 이유는 RPN이 이미 계산된 feature
              map을 입력으로 받는 2개 정도의 작은 conv layer이기 때문이다. 따라서 원본 이미지 전체를 다루는 selective
              search 등의 방법보다 훨씬 효율적이다.
            </p>

            <h2>Dealing with Scale: Feature Pyramid Network</h2>
            <p>
              한 이미지에는 같은 물체더라도 위치에 따라 크게 보일 수도, 작게 보일 수도 있다. 이러한 scale에 대한
              invariance를 얻기 위해, 다양한 크기의 물체를 학습시키기 위한 방법이 등장했다.
            </p>
            <p>
              우선 이미지 자체를 다양한 scale로 resize한 다음 각각의 resolution에 맞춰진 CNN을 학습시키는 방법이 있지만
              이는 매우 비효율적이다.
            </p>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_14/img14.png"
              alt="Multiscale features"
            />
            <p>
              이보다 나은 방법으로 Multiscale features를 이용하는 방법이 있다. 이는 CNN의 각 stage에서의 중간 output을
              이용하는 방법으로, 각 stage를 거칠 때마다 이미지의 resolution이 절반씩 줄어드는 설정을 이용하였다. 이러한
              output들을 크기별 RPN에 입력한 후 출력되는 RoI들을 한데 모아 per-region network에 입력시킨다. 그러나, 초기
              layer에서 나온 output은 high-level feature를 포함하지 않는다는 문제가 있다.
            </p>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_14/img15.png"
              alt="Feature pyramid network"
            />
            <p>
              이 문제를 해결하기 위한 방법으로 Feature Pyramid Network(FPN)이 제안되었다. 원본 이미지를 네트워크에
              통과시키며 중간 stage의 output을 저장해 두고, 마지막 stage의 output까지 얻는다. 위 그림에서 7x7 크기인
              최종 output은 바로 RPN과 같은 object detector에 입력한다. 중간 stage output의 경우 상위 stage output과
              순차적으로 더해지며 high-level feature를 포함하게 된다. 이때 upsampling이나 1x1 conv 등 shape의 조절을
              거친다. 이후 각 feature map의 크기에 맞는 별개의 RPN에 입력한다. 이 과정을 통해 모든 크기의 feature map을
              전체 네트워크를 이용해 연산한 것과 같은 효과를 얻을 수 있다. FPN은 추가적으로 소요되는 연산이 매우 적기
              때문에 scaling 문제를 해결하기 위해 거의 대부분의 모델에 사용되는 방법이다.
            </p>
          </div>
          <div class="post-footer">
            <div class="post-footer-profile">
              <img src="/asset/cover.jpg" alt="Seongmin Jung" />
              <h1>Seongmin Jung</h1>
            </div>
            <h2>
              Other posts in
              <a href="/study/deep_learning_for_computer_vision.html">Deep Learning for Computer Vision</a> series
            </h2>
            <ul>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_14.html">
                  <span><b>Lecture 14: Object Detectors</b></span>
                  <time datetime="2024-05-19">May 19, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_13.html">
                  <span>Lecture 13: Object Detection</span>
                  <time datetime="2024-05-17">May 17, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_12.html">
                  <span>Lecture 12: Deep Learning Software</span>
                  <time datetime="2024-05-15">May 15, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_11.html">
                  <span>Lecture 11: CNN Architectures II</span>
                  <time datetime="2024-05-12">May 12, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_10.html">
                  <span>Lecture 10: Training Neural Networks II</span>
                  <time datetime="2024-05-11">May 11, 2024</time>
                </a>
              </li>
            </ul>
          </div>
        </section>
      </div>
    </main>

    <footer>
      <p>&copy; 2024 Seongmin Jung<br />Designed and developed by Seongmin Jung</p>
    </footer>
  </body>
</html>
