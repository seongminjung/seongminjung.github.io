<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 9: Training Neural Networks I</title>
    <meta name="description" content="Seongmin Jung's personal website" />
    <meta name="keywords" content="Seongmin Jung, Robotics" />
    <meta name="author" content="Seongmin Jung" />
    <meta name="language" content="English" />
    <link rel="shortcut icon" href="/favicon.png" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LL44K1WZ0G"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-LL44K1WZ0G");
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

    <!-- Load an icon library to show a hamburger menu (bars) on small screens -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <!-- import css -->
    <link rel="stylesheet" href="/css/index.css" />
    <!-- import js -->
    <script src="/js/script.js" defer></script>
  </head>

  <body>
    <header>
      <img class="cover" src="/asset/cover.jpg" alt="Seongmin Jung" />
      <h1 class="title"><a href="/index.html">Seongmin Jung</a></h1>
      <nav class="nav">
        <a href="/index.html">Home</a>
        <a href="/projects.html">Projects</a>
        <a href="/study.html" class="bold">Study</a>
        <a href="/blog.html">Blog</a>
        <i id="nav-toggle" class="nav-toggle fa fa-bars" onclick="toggleNav()"></i>
      </nav>
    </header>

    <div id="nav-modal-bg" onclick="toggleNav()"></div>
    <nav id="nav-modal">
      <a href="/index.html">Home</a>
      <a href="/projects.html">Projects</a>
      <a href="/study.html" class="bold">Study</a>
      <a href="/blog.html">Blog</a>
    </nav>

    <main>
      <div class="container">
        <section id="post">
          <a id="series-tag" href="/study/deep_learning_for_computer_vision.html"
            ><i class="fa fa-book"></i> Deep Learning for Computer Vision</a
          >
          <h1>Lecture 9: Training Neural Networks I</h1>
          <p class="date">Posted on <time datetime="2024-05-03">May 3, 2024</time></p>
          <div class="post-body">
            <h2>Overview</h2>
            <p>
              지금까지 뉴럴 네트워크의 구성 요소와 학습 방법에 대해 알아보았다. 이번 강의에서는 뉴럴 네트워크의 학습에
              필요한 구체적인 방법론들에 대해 알아보도록 하자. 크게 아래의 세 가지로 분류할 수 있다.
            </p>
            <ol>
              <li>One time setup</li>
              <ul>
                <li>Activation functions</li>
                <li>Data preprocessing</li>
                <li>Weight initialization</li>
                <li>Regularization</li>
              </ul>
              <li>Training dynamics</li>
              <ul>
                <li>Learning rate schedules</li>
                <li>Large-batch training</li>
                <li>Hyperparameter optimization</li>
              </ul>
              <li>After training</li>
              <ul>
                <li>Model ensembles</li>
                <li>Transfer learning</li>
              </ul>
            </ol>
            <p>
              이번 강의에서는 1. One time setup에 대해 알아본다. Training 전에 어떻게 네트워크를 세팅할 것인지에 대한
              부분이다.
            </p>

            <h2>Activation functions</h2>
            <p>Activation function의 종류는 아래 그림과 같이 Sigmoid, tanh, ReLU, Leaky ReLU, ELU, GELU 등이 있다.</p>
            <img src="/study/deep_learning_for_computer_vision/lecture_9/img1.png" alt="Activation functions" />

            <h3>Sigmoid</h3>
            <p>Sigmoid 함수는 아래와 같이 정의된다.</p>
            <p class="math-center">$f(x) = \cfrac{1}{1 + e^{-x}}$</p>
            <p>
              Sigmoid는 입력값을 (0, 1)의 범위 안에 압축시키는 효과가 있다. 이를 통해 모델의 출력을 확률 형태로 표현할
              수 있다는 특징이 있다. 하지만 Sigmoid는 여러 단점을 가지고 있어 오늘날 거의 사용되지 않는다.
            </p>
            <p>
              첫 번째 단점으로는, Sigmoid는 saturate하는 경향이 있다. 즉, 입력값이 크거나 작을 때 각각 1과 0으로
              수렴하기 때문에 gradient가 0에 가까워지는 vanishing gradient 문제가 발생한다.
            </p>
            <p class="math-center">
              $\cfrac{\partial L}{\partial x} = \cfrac{\partial \sigma}{\partial x} \cdot \cfrac{\partial L}{\partial
              \sigma}$
            </p>
            <p>
              위와 같이 backpropagation이 진행될수록 0에 가까운 gradient $\frac{\partial \sigma}{\partial x}$가 점점
              곱해지기 때문에 결국 초반 layer들은 gradient를 제대로 받지 못하게 된다. 이로 인해 앞쪽은 학습이 잘 되지
              않는 문제가 발생한다. 이를 vanishing gradient problem이라고 한다. 결국 앞쪽이 잘 학습되어야 뒤쪽이 의미가
              있기 때문에 이는 모델의 성능에 심각한 영향을 미친다.
            </p>
            <p class="math-center">
              $\cfrac{\partial L}{\partial w} = \cfrac{\partial \sigma}{\partial w} \cdot \cfrac{\partial L}{\partial
              \sigma}$
            </p>
            <p>
              두 번째 단점으로는, Sigmoid는 zero-centered하지 않다는 점이다. 아래 수식은 $w$에 대한 gradient를 구하는
              과정인데, 중간 변수로 $\sigma(wx)$ 대신 $wx$를 사용하여 전개하였다.
            </p>
            <p class="math-center">
              $\begin{align*} \cfrac{\partial L}{\partial w} & = \cfrac{\partial (wx)}{\partial w} \cdot \cfrac{\partial
              L}{\partial (wx)} \\ & = x \cdot \cfrac{\partial L}{\partial (wx)} \end{align*}$
            </p>
            <p>
              즉 $w$에 대한 gradient는 이전 레이어로부터의 입력 $x$와 upstream gradient에 의해 결정되는데, Sigmoid는
              항상 양수이기 때문에 $x$의 모든 element가 양수가 되고, 결국 $\frac{\partial L}{\partial (wx)}$의 부호에
              따라 $w$의 모든 element가 같은 방향으로 업데이트된다. 이는 모델의 학습을 더 어렵게 만드는 요인이 된다.
            </p>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_9/img2.png"
              alt="Sigmoid gradient"
            />
            <p>
              위 그림은 $w_1$과 $w_2$를 각 축으로 하여 $w$의 변화를 화살표로 나타낸 것인데, $\nabla w_1$과 $\nabla
              w_2$가 항상 같은 부호를 가져야 하는 경우 $\nabla w$는 항상 1사분면 또는 3사분면을 향하고 있을 수밖에 없다.
              따라서 $w$는 한번에 파란색 화살표처럼 2사분면 또는 4사분면 방향으로 이동할 수 없고 빨간색 화살표처럼
              지그재그 형태의 경로로 이동할 수밖에 없다. 이로 인해 optimization이 느려지는 문제가 발생하게 된다.
            </p>
            <p>
              다만 실제 training은 mini-batch로 진행되기 때문에 여러 방향의 gradient가 합쳐져 실제로는 모든 방향의
              gradient를 표현할 수 있고, 또한 Batch Normalization을 사용하여 해결할 수 있기 때문에 심각한 문제는 아니다.
            </p>
            <p>
              세 번째 작은 단점으로는 지수함수가 다른 연산보다 연산량이 많다는 점인데, activation function은 conv
              layer나 FC layer에 비교하면 연산량이 극히 일부분이기 때문에 큰 문제가 되지 않는다.
            </p>

            <h3>Tanh</h3>
          </div>
          <div class="post-footer">
            <div class="post-footer-profile">
              <img src="/asset/cover.jpg" alt="Seongmin Jung" />
              <h1>Seongmin Jung</h1>
            </div>
            <h2>
              Other posts in
              <a href="/study/deep_learning_for_computer_vision.html">Deep Learning for Computer Vision</a> series
            </h2>
            <ul>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_9.html">
                  <span><b>Lecture 9: Training Neural Networks I</b></span>
                  <time datetime="2024-05-03">May 3, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_8.html">
                  <span>Lecture 8: CNN Architectures I</span>
                  <time datetime="2024-04-30">April 30, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_7.html">
                  <span>Lecture 7: Convolutional Networks</span>
                  <time datetime="2024-04-27">April 27, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_6.html">
                  <span>Lecture 6: Backpropagation</span>
                  <time datetime="2024-04-20">April 20, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_5.html">
                  <span>Lecture 5: Neural Networks</span>
                  <time datetime="2024-04-18">April 18, 2024</time>
                </a>
              </li>
            </ul>
          </div>
        </section>
      </div>
    </main>

    <footer>
      <p>&copy; 2024 Seongmin Jung<br />Designed and developed by Seongmin Jung</p>
    </footer>
  </body>
</html>
