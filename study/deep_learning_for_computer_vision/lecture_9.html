<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Lecture 9: Training Neural Networks I</title>
    <meta name="description" content="Seongmin Jung's personal website" />
    <meta name="keywords" content="Seongmin Jung, Robotics" />
    <meta name="author" content="Seongmin Jung" />
    <meta name="language" content="English" />
    <link rel="shortcut icon" href="/favicon.png" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LL44K1WZ0G"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-LL44K1WZ0G");
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
      });
    </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

    <!-- Load an icon library to show a hamburger menu (bars) on small screens -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" />
    <!-- import css -->
    <link rel="stylesheet" href="/css/index.css" />
    <!-- import js -->
    <script src="/js/script.js" defer></script>
  </head>

  <body>
    <header>
      <img class="cover" src="/asset/cover.jpg" alt="Seongmin Jung" />
      <h1 class="title"><a href="/index.html">Seongmin Jung</a></h1>
      <nav class="nav">
        <a href="/index.html">Home</a>
        <a href="/projects.html">Projects</a>
        <a href="/study.html" class="bold">Study</a>
        <a href="/blog.html">Blog</a>
        <i id="nav-toggle" class="nav-toggle fa fa-bars" onclick="toggleNav()"></i>
      </nav>
    </header>

    <div id="nav-modal-bg" onclick="toggleNav()"></div>
    <nav id="nav-modal">
      <a href="/index.html">Home</a>
      <a href="/projects.html">Projects</a>
      <a href="/study.html" class="bold">Study</a>
      <a href="/blog.html">Blog</a>
    </nav>

    <main>
      <div class="container">
        <section id="post">
          <a id="series-tag" href="/study/deep_learning_for_computer_vision.html"
            ><i class="fa fa-book"></i> Deep Learning for Computer Vision</a
          >
          <h1>Lecture 9: Training Neural Networks I</h1>
          <p class="date">Posted on <time datetime="2024-05-03">May 3, 2024</time></p>
          <div class="post-body">
            <h2>Overview</h2>
            <p>
              지금까지 뉴럴 네트워크의 구성 요소와 학습 방법에 대해 알아보았다. 이번 강의에서는 뉴럴 네트워크의 학습에
              필요한 구체적인 방법론들에 대해 알아보도록 하자. 크게 아래의 세 가지로 분류할 수 있다.
            </p>
            <ol>
              <li>One time setup</li>
              <ul>
                <li>Activation functions</li>
                <li>Data preprocessing</li>
                <li>Weight initialization</li>
                <li>Regularization</li>
              </ul>
              <li>Training dynamics</li>
              <ul>
                <li>Learning rate schedules</li>
                <li>Large-batch training</li>
                <li>Hyperparameter optimization</li>
              </ul>
              <li>After training</li>
              <ul>
                <li>Model ensembles</li>
                <li>Transfer learning</li>
              </ul>
            </ol>
            <p>
              이번 강의에서는 1. One time setup에 대해 알아본다. Training 전에 어떻게 네트워크를 세팅할 것인지에 대한
              부분이다.
            </p>

            <h2>Activation functions</h2>
            <p>Activation function의 종류는 아래 그림과 같이 Sigmoid, tanh, ReLU, Leaky ReLU, ELU, GELU 등이 있다.</p>
            <img src="/study/deep_learning_for_computer_vision/lecture_9/img1.png" alt="Activation functions" />

            <h3>Sigmoid</h3>
            <p>Sigmoid 함수는 아래와 같이 정의된다.</p>
            <p class="math-center">$f(x) = \cfrac{1}{1 + e^{-x}}$</p>
            <p>
              Sigmoid는 입력값을 (0, 1)의 범위 안에 압축시키는 효과가 있다. 이를 통해 모델의 출력을 확률 형태로 표현할
              수 있다는 특징이 있다. 하지만 Sigmoid는 여러 단점을 가지고 있어 오늘날 거의 사용되지 않는다.
            </p>
            <p>
              첫 번째 단점으로는, Sigmoid는 saturate하는 경향이 있다. 즉, 입력값이 크거나 작을 때 각각 1과 0으로
              수렴하기 때문에 gradient가 0에 가까워지는 문제가 발생한다. 이를 <b>Vanishing Gradient Problem</b>이라고
              한다.
            </p>
            <p class="math-center">
              $\cfrac{\partial L}{\partial x} = \cfrac{\partial \sigma}{\partial x} \cdot \cfrac{\partial L}{\partial
              \sigma}$
            </p>
            <p>
              위와 같이 backpropagation이 진행될수록 0에 가까운 gradient $\frac{\partial \sigma}{\partial x}$가 점점
              곱해지기 때문에 결국 초반 layer들은 gradient를 제대로 받지 못하게 된다. 이로 인해 앞쪽은 학습이 잘 되지
              않는 문제가 발생한다. 이를 vanishing gradient problem이라고 한다. 결국 앞쪽이 잘 학습되어야 뒤쪽이 의미가
              있기 때문에 이는 모델의 성능에 심각한 영향을 미친다.
            </p>
            <p class="math-center">
              $\cfrac{\partial L}{\partial w} = \cfrac{\partial \sigma}{\partial w} \cdot \cfrac{\partial L}{\partial
              \sigma}$
            </p>
            <p>
              두 번째 단점으로는, Sigmoid는 zero-centered하지 않다는 점이다. 아래 수식은 $w$에 대한 gradient를 구하는
              과정인데, 중간 변수로 $\sigma(wx)$ 대신 $wx$를 사용하여 전개하였다.
            </p>
            <p class="math-center">
              $\begin{align*} \cfrac{\partial L}{\partial w} & = \cfrac{\partial (wx)}{\partial w} \cdot \cfrac{\partial
              L}{\partial (wx)} \\ & = x \cdot \cfrac{\partial L}{\partial (wx)} \end{align*}$
            </p>
            <p>
              즉 $w$에 대한 gradient는 이전 레이어로부터의 입력 $x$와 upstream gradient에 의해 결정되는데, Sigmoid는
              항상 양수이기 때문에 $x$의 모든 element가 양수가 되고, 결국 $\frac{\partial L}{\partial (wx)}$의 부호에
              따라 $w$의 모든 element가 같은 방향으로 업데이트된다. 이는 모델의 학습을 더 어렵게 만드는 요인이 된다.
            </p>
            <img
              class="half"
              src="/study/deep_learning_for_computer_vision/lecture_9/img2.png"
              alt="Sigmoid gradient"
            />
            <p>
              위 그림은 $w_1$과 $w_2$를 각 축으로 하여 $w$의 변화를 화살표로 나타낸 것인데, $\nabla w_1$과 $\nabla
              w_2$가 항상 같은 부호를 가져야 하는 경우 $\nabla w$는 항상 1사분면 또는 3사분면을 향하고 있을 수밖에 없다.
              따라서 $w$는 한번에 파란색 화살표처럼 2사분면 또는 4사분면 방향으로 이동할 수 없고 빨간색 화살표처럼
              지그재그 형태의 경로로 이동할 수밖에 없다. 이로 인해 optimization이 느려지는 문제가 발생하게 된다.
            </p>
            <p>
              다만 실제 training은 mini-batch로 진행되기 때문에 여러 방향의 gradient가 합쳐져 실제로는 모든 방향의
              gradient를 표현할 수 있고, 또한 Batch Normalization을 사용하여 해결할 수 있기 때문에 심각한 문제는 아니다.
            </p>
            <p>
              세 번째 작은 단점으로는 지수함수가 다른 연산보다 연산량이 많다는 점인데, activation function은 conv
              layer나 FC layer에 비교하면 연산량이 극히 일부분이기 때문에 큰 문제가 되지 않는다.
            </p>

            <h3>Tanh</h3>
            <p>Tanh 함수는 아래와 같이 정의된다.</p>
            <p class="math-center">$f(x) = \tanh(x) = \cfrac{e^x - e^{-x}}{e^x + e^{-x}}$</p>
            <p>
              Tanh 함수는 Sigmoid 함수를 y축 방향으로 단순히 scale과 shift한 것으로, 인풋을 -1과 1 사이의 값으로
              압축시킨다. 따라서 zero-centered되어 있다는 장점이 있지만 그 외 특성은 모두 Sigmoid와 동일하다. 따라서
              Sigmoid와 마찬가지로 오늘날 거의 쓰이지 않는다.
            </p>

            <h3>ReLU (Rectified Linear Unit)</h3>
            <p>ReLU 함수는 아래와 같이 정의된다.</p>
            <p class="math-center">$f(x) = \max(0, x)$</p>
            <p>
              ReLU 함수는 $x$가 양수일 때 saturate하지 않는다는 장점이 있고 연산도 매우 효율적이다. 또한 Sigmoid나
              tanh보다 6배 정도 빠르게 수렴한다고 알려져 있다. 다만 ReLU는 zero-centered되어 있지 않으며 모든 출력값이
              양수라는 단점도 가지고 있다.
            </p>
            <p>
              그보다 더 큰 문제는 $x$가 음수일 때 gradient가 0이 되는 문제이다. 이로 인해 출력이 음수인 뉴런의 경우 전혀
              학습을 하지 못하는데, 이를 <b>Dead ReLU Problem</b>이라고 한다. 데이터셋의 모든 이미지에 대해 뉴런의
              출력값이 음수일 경우 그 뉴런은 어떤 mini-batch를 사용하더라도 전혀 학습하지 못한다. Dead neuron은 연산은
              그대로 하지만 의미 있는 출력을 내보내지 못해 연산량의 낭비를 가져온다. 하지만 Batch Normalization 등을
              사용하기 때문에 이 문제는 실제로는 잘 발생하지 않는다.
            </p>

            <h3>Leaky ReLU</h3>
            <p>Leaky ReLU 함수는 아래와 같이 정의된다.</p>
            <p class="math-center">$f(x) = \max(\alpha x, x)$</p>
            <p>
              Leaky ReLU는 ReLU의 Dead ReLU Problem을 해결하기 위해 나온 함수로, $x$가 음수일 때 gradient가 0이 되는
              문제를 해결한다. $\alpha$는 일반적으로 0.1로 설정한다. $x$가 음수일 때에도 gradient가 0이 되지 않기 때문에
              saturate되거나 Dead ReLU Problem이 발생하지 않는다.
            </p>
            <p>
              Leaky ReLU의 한 가지 변형으로 Parametric ReLU가 있다. 이는 $\alpha$를 학습 가능한 파라미터로 설정한
              것인데, backpropagation을 통해 최적의 $\alpha$를 찾는다.
            </p>

            <h3>ELU (Exponential Linear Unit)</h3>
            <p>ELU 함수는 아래와 같이 정의된다.</p>
            <p class="math-center">
              $f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha (e^x - 1) & \text{if } x \leq 0 \end{cases}$
            </p>
            <p>
              ELU는 ReLU의 형태와 유사하지만 음수 부분을 부드럽게 만든 함수이다. ReLU의 장점을 그대로 가지며, 추가적으로
              output의 평균을 조금 더 0에 가깝게 할 수 있으며 noise에 조금 더 robust하다. 다만 음수에서 saturate하며
              지수함수 연산이 있기 때문에 ReLU보다 연산량이 많은 편이라는 단점이 있다.
            </p>

            <h3>SELU (Scaled Exponential Linear Unit)</h3>
            <p>SELU 함수는 아래와 같이 정의된다.</p>
            <p class="math-center">
              $f(x) = \begin{cases} \lambda x & \text{if } x > 0 \\ \lambda\alpha (e^x - 1) & \text{if } x \leq 0
              \end{cases}$
            </p>
            <p class="math-center">$\alpha = 1.673263242354..., \quad \lambda = 1.050700987355...$</p>
            <p>
              SELU는 ELU의 scale된 버전이다. ELU와 비슷하지만 $\alpha$와 $\lambda$가 위의 특정 값일 때
              "Self-Normalizing"되는 특성을 가지는데, 이는 Batch Normalization 없이도 깊은 네트워크를 트레이닝할 수 있게
              하는 특성이다. 하지만 실제로 잘 사용되는 activation function은 아니다.
            </p>

            <h3>GELU (Gaussian Error Linear Unit)</h3>
            <p>GELU 함수는 아래와 같이 정의된다.</p>
            <p class="math-center">
              $\begin{gather} X \sim N(0, 1) \\ f(x) = xP(X \leq x) = \cfrac{x}{2}(1 + \text{erf}(\cfrac{x}{\sqrt{2}}))
              \end{gather}$
            </p>
            <p>
              이때, $X$가 Gaussian 분포를 따르므로 $x$가 클수록 $P(X \leq x)$는 1에 가까워질 것이고 작을수록 0에
              가까워질 것이다. 따라서 ReLU와 비슷하면서도 부드러운 곡선의 형태를 가진다. Transformer에서 주로 사용된다.
            </p>
            <p>
              GELU의 한 가지 특징으로는 non-monotonic하다는 점인데, 즉 $x$가 음수일 때 그래프가 아래로 튀어나온 부분이
              있다. 따라서 두 개의 $x$값이 동일한 출력을 내는 경우가 생긴다. 이 경우, 그 다음 node가 원래 입력이 정확히
              무엇인지 모른 채로 추론을 진행해야 하므로 약간의 stochasticity를 가진다. 이를 통해 약간의 regularization
              효과를 얻을 수 있다.
            </p>
            <img
              src="/study/deep_learning_for_computer_vision/lecture_9/img3.png"
              alt="Activation function performances"
            />
            <p>
              위 그림은 각 activation function의 성능을 여러 모델에서 비교한 결과이다. 이때, 수치를 보면 Sigmoid와
              tanh를 제외하면 어떤 activation function을 쓰더라도 차이가 1~2퍼센트 이상 크지 않다. 따라서 activation
              function을 고르는 데 시간을 너무 많이 쓰기보다는 ReLU를 사용하여 우선 진행하고, 최종적으로 마지막 0.1%의
              성능을 끌어내야 할 때 여러 activation function을 비교하는 것이 좋다.
            </p>

            <h2>Data Preprocessing</h2>
          </div>
          <div class="post-footer">
            <div class="post-footer-profile">
              <img src="/asset/cover.jpg" alt="Seongmin Jung" />
              <h1>Seongmin Jung</h1>
            </div>
            <h2>
              Other posts in
              <a href="/study/deep_learning_for_computer_vision.html">Deep Learning for Computer Vision</a> series
            </h2>
            <ul>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_9.html">
                  <span><b>Lecture 9: Training Neural Networks I</b></span>
                  <time datetime="2024-05-03">May 3, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_8.html">
                  <span>Lecture 8: CNN Architectures I</span>
                  <time datetime="2024-04-30">April 30, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_7.html">
                  <span>Lecture 7: Convolutional Networks</span>
                  <time datetime="2024-04-27">April 27, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_6.html">
                  <span>Lecture 6: Backpropagation</span>
                  <time datetime="2024-04-20">April 20, 2024</time>
                </a>
              </li>
              <li>
                <a href="/study/deep_learning_for_computer_vision/lecture_5.html">
                  <span>Lecture 5: Neural Networks</span>
                  <time datetime="2024-04-18">April 18, 2024</time>
                </a>
              </li>
            </ul>
          </div>
        </section>
      </div>
    </main>

    <footer>
      <p>&copy; 2024 Seongmin Jung<br />Designed and developed by Seongmin Jung</p>
    </footer>
  </body>
</html>
